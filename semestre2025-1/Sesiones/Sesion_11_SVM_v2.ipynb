{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/CienciaDatosUdea/002_EstudiantesAprendizajeEstadistico/blob/main/semestre2025-1/Sesiones/Sesion_11_SVM_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Máquinas de Soporte Vectorial SVM\n",
    "\n",
    "Sección 2.4.3 en https://arxiv.org/pdf/2204.04198\n",
    "\n",
    "Este es un modelo con origenes que se remontan a los 60's y que se creó desde un punto de vista geométrico. Con este se puede llegar por un camino diferente a la función de coste y regularización a los que llegamos por el método Maximum Likelihood Estimator.\n",
    "\n",
    "Empecemos con un problema de clasificación de unos datos cuyo i-esimos record $X^{(i)}$ tiene dos variables y que queremos predecir si pertenecen a una clase +1 y 0. El modelo de clasificación se condensa en tres parámetros $\\theta_0,\\theta_1,\\theta_2$ que definen una linea (un hiperplano cuando pensemos en más dimensiones) a través de la ecuación $\\theta^T X + \\theta_0 = 0$ con $ \\theta=(\\theta_1,\\theta_2)$. Los puntos de la linea son los puntos perpendiculares a $ \\theta$. Los puntos del plano quedan clasificados por la región respecto por el signo que resulta del producto escalar $\\theta^T X$. Si este producto es positivo están en la región que definimos como de clase +1 y si es negativo en la región definida como clase 0. Con solo argumentos de geometría se demuestra que encontrar los mejores parámetros para un conjunto de datos equivale a minimizar una función de coste parecida a las encontradas anteriormente.\n",
    "\n",
    "La cantidad principal es la menor distancia que hay entre un punto cualquiera del plano (x1,x2) y el plano del modelo $\\theta^T X + \\theta_0 = 0$. Esta distancia está dada por $$ d(x, \\theta) = \\frac{\\theta^T x + \\theta_0}{| \\theta |} $$\n",
    "El problema de encontrar un buen hiperplano de clasificación consiste entonces en encontrar un conjunto de parámetros que la distancia al hiperplano sea tan grande como pósible o al menos mas grande que una región que escribiremos como $M | \\theta |$ es decir resolver que para todo i se cumpla:\n",
    "$$ y_i (\\theta^T x + \\theta_0) \\ge  M | \\theta | $$\n",
    "\n",
    "Ahora, para simplificar, reescalamos $\\theta$ con $ | \\theta | = {1}{M}$ tal que la el problema a solucionar se convierte en $$ y_i (\\theta^T x + \\theta_0) \\ge  1 $$\n",
    "\n",
    "Este reescalamiento hace que buscar que M sea tan grande como posible se convierte en el problema de buscar $| \\theta |$ sea tan pequeño como posible. El problema de encontrar el mejor modelo de clasificiación es ahora entonces minimizar $| \\theta |$ con la condición que $ y_i (\\theta^T x + \\theta_0) \\ge  1 $ para todo i. Esto se logra por el método de Lagrange. Buscamos minimizar:\n",
    "\n",
    "$$ L = \\frac{1}{2} | \\theta |^2 - \\sum_i [ \\alpha_i  y_i (\\theta^T x + \\theta_0) -  1 ] $$\n",
    "\n",
    "Observermos que los terminos se parecen a los que llegamos por MLE. El primero disminuye la complejidad y el segundo término depende del error que se comete en la clasificación. En una libreria como sklearn la siguiente función de coste se llamaría con un parámetro $C$ que multiplicaría el segundo termino. Con esto podemos graduar que tanta importancia se le da al segundo término sobre el primero:\n",
    "\n",
    "Hay mas detalles técnicos a tener en cuenta que si lo necesitan pueden consultar en estas notas por ejemplo:\n",
    "https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eh6C_5X3AiRG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Libraries for draw contours\n",
    "def make_meshgrid(x, y, h=0.02):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "\n",
    "def plot_contoursExact(ax, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "mAm2pfKHCMA5",
    "outputId": "d07003ff-2eb9-4567-c6bf-9cdfe04006a5"
   },
   "outputs": [],
   "source": [
    "# Dataset Toys References\n",
    "# https://scikit-learn.org/stable/datasets/toy_dataset.html\n",
    "# https://matplotlib.org/stable/gallery/subplots_axes_and_figures/subplots_demo.html\n",
    "# Dataset sinteticos\n",
    "X0, y0 = make_classification(\n",
    "    n_features=2, n_redundant=0, n_informative=2, random_state=1, \n",
    "    n_clusters_per_class=1\n",
    ")\n",
    "X1, y1 = make_moons(n_samples=100, noise=0.15, shuffle=True,  random_state=1)\n",
    "X2, y2 = make_circles(n_samples=100, noise=0.05, shuffle=True,  random_state=1)\n",
    "X3, y3 = make_blobs(n_samples=500, centers=3, n_features=2,shuffle=True, \n",
    "                    random_state=10)\n",
    "\n",
    "fig, axs = plt.subplots(2,2)\n",
    "\n",
    "axs[0, 0].plot(X0[:,0][y0==0],X0[:,1][y0==0],\"ro\", alpha=0.5)\n",
    "axs[0, 0].plot(X0[:,0][y0==1],X0[:,1][y0==1],\"bo\", alpha=0.5)\n",
    "\n",
    "# Dataset a moons\n",
    "axs[0, 1].plot(X1[:,0][y1==0],X1[:,1][y1==0],\"ro\", alpha=0.5)\n",
    "axs[0, 1].plot(X1[:,0][y1==1],X1[:,1][y1==1],\"bo\", alpha=0.5)\n",
    "\n",
    "# Dataset circles\n",
    "axs[1, 0].plot(X2[:,0][y2==0],X2[:,1][y2==0],\"ro\", alpha=0.5)\n",
    "axs[1, 0].plot(X2[:,0][y2==1],X2[:,1][y2==1],\"bo\", alpha=0.5)\n",
    "\n",
    "# Dataset circles\n",
    "axs[1, 1].plot(X3[:,0][y3==0],X3[:,1][y3==0],\"ro\", alpha=0.5)\n",
    "axs[1, 1].plot(X3[:,0][y3==1],X3[:,1][y3==1],\"bo\", alpha=0.5)\n",
    "axs[1, 1].plot(X3[:,0][y3==2],X3[:,1][y3==2],\"go\", alpha=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1zgIVybCa16"
   },
   "outputs": [],
   "source": [
    "# Based on :\n",
    "# https://rramosp.github.io/ai4eng.v1.20211.udea/content/NOTES%2003.03%20-%20SVM%20AND%20FEATURE%20TRANSFORMATION.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "QeGCBy_NI8cG",
    "outputId": "e8a0e844-d143-4fa7-b631-9a2f0de4b10a"
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1, shuffle=True)\n",
    "#X, y = make_circles(n_samples=200, noise=0.1, shuffle=True,  random_state=1)\n",
    "\n",
    "plt.plot(X[:,0][y==0],X[:,1][y==0],\"ro\", alpha=0.5)\n",
    "plt.plot(X[:,0][y==1],X[:,1][y==1],\"bo\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "QLVNZZ8KNLb5",
    "outputId": "c808713a-d2de-4682-9dcf-daced322c56d"
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel= 'linear', C=1.0) # C denota la fuerza del segundo termino. C muy pequeño hace que minimizar theta es mas importante\n",
    "# se vuelven cero, o una constante y a partir de cierto valor ya el segundo termino empiza a tener mas peso \n",
    "clf.fit(X, y)\n",
    "\n",
    "#Countour plot\n",
    "fig, ax = plt.subplots()\n",
    "X0, X1 = X[:, 0], X[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "plt.plot(X[y==0][:,0],X[y==0][:,1],\"bo\", alpha=1)\n",
    "plt.plot(X[y==1][:,0],X[y==1][:,1],\"ro\", alpha=1)\n",
    "print(f\"Training error:{clf.score(X, y):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos de Kernel\n",
    "\n",
    "Los métodos de Kernel transforman los datos en un espacio en el cual ellos tienen posibilidad de ser separados. Por ejemplo los circulos no hay manera que una frontera lineal los separe. Mas concretamente se trata de hacer una transformación $x \\to \\phi(x)$. Se determina una nueva función de coste que en vez de depender de $x_i$ va a depender de $\\phi(x_i)$. El nombre de método de kernel viene del hecho del reemplazo\n",
    "$$  \\ket x_i, x_j \\bra \\to \\ket \\phi(x_i), \\phi(x_j) = K(x_i, x_j) $$\n",
    "donde se denota el kernel. Vamos a elegir diferentes kernels para los problemas anteriores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=200, noise=0.1, shuffle=True,  random_state=1)\n",
    "clf = SVC(kernel= 'rbf', gamma=1) # El parametro gamma es un parametro relacionado con la complejidad del kernel\n",
    "clf.fit(X, y)\n",
    "\n",
    "#Countour plot\n",
    "fig, ax = plt.subplots()\n",
    "X0, X1 = X[:, 0], X[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "plt.plot(X[y==0][:,0],X[y==0][:,1],\"bo\", alpha=1)\n",
    "plt.plot(X[y==1][:,0],X[y==1][:,1],\"ro\", alpha=1)\n",
    "print(f\"Training error:{clf.score(X, y):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVPYFpygOgzV"
   },
   "source": [
    "# Grid Search of parameter in SVM\n",
    "\n",
    "Grid-search is used to find the optimal hyperparameters of a model which results in the most ‘accurate’ predictions.\n",
    "https://towardsdatascience.com/grid-search-for-model-tuning-3319b259367e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "wxzF0V-CRNtD",
    "outputId": "833eddec-14a0-4ec9-fa0b-5bf5cec352a0"
   },
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "X, y = make_circles(n_samples=200, noise=0.1, shuffle=True,  random_state=1)\n",
    "\n",
    "plt.plot(X[:,0][y==0],X[:,1][y==0],\"ro\", alpha=0.5)\n",
    "plt.plot(X[:,0][y==1],X[:,1][y==1],\"bo\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vlZCTfBkKtdI",
    "outputId": "7b6be882-fe82-4982-de98-28b787ee13df"
   },
   "outputs": [],
   "source": [
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "\n",
    "clf = GridSearchCV(estimator=SVC(),\n",
    "             param_grid = parameters)\n",
    "\n",
    "clf.fit(X, y)\n",
    "sorted(clf.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Os8epCfESHpm",
    "outputId": "e3c06c75-fe93-4a42-aa5f-f50db6d90da0"
   },
   "outputs": [],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TFyWAaigSTem",
    "outputId": "eb7c93bf-4309-4c4e-e614-3eaa0e08ee42"
   },
   "outputs": [],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rwm2fzrnT76f",
    "outputId": "ecea7f7c-0fec-4612-c85e-b477ca705fbc"
   },
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-cviANiUnrY",
    "outputId": "e29b4851-6a85-4554-9af6-04bf5bfaa7ba"
   },
   "outputs": [],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "h9W_ZMV2VOuH",
    "outputId": "fc043704-627a-4fe3-b4bd-200e3fa20df9"
   },
   "outputs": [],
   "source": [
    "#Countour plot\n",
    "fig, ax = plt.subplots()\n",
    "X0, X1 = X[:, 0], X[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "plt.plot(X[y==0][:,0],X[y==0][:,1],\"bo\", alpha=1)\n",
    "plt.plot(X[y==1][:,0],X[y==1][:,1],\"ro\", alpha=1)\n",
    "print(f\"Training error:{clf.score(X, y):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "X, y = make_circles(n_samples=1000, noise=0.1, shuffle=True,  random_state=1)\n",
    "\n",
    "#Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1080)\n",
    "\n",
    "#Scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search\n",
    "parameters = {'kernel':('linear', 'rbf','poly'), 'C':[0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "\n",
    "classifier = GridSearchCV(estimator = SVC(),\n",
    "                          param_grid = parameters,\n",
    "                          scoring='accuracy')\n",
    "classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Grid search plots and print\n",
    "mean_scores = classifier.cv_results_['mean_test_score']\n",
    "std_scores = classifier.cv_results_['std_test_score']\n",
    "kernel = classifier.cv_results_['param_kernel']\n",
    "c = classifier.cv_results_['param_C'].data\n",
    "\n",
    "scores = {'C': c, 'kernel':kernel, 'mean_test_score': mean_scores, 'std_test_score': std_scores}\n",
    "scores_df = pd.DataFrame(data = scores)\n",
    "\n",
    "ax = sns.relplot(data=scores_df,\n",
    "                 x='C',\n",
    "                 y='mean_test_score',\n",
    "                 marker='X',\n",
    "                 hue='kernel',\n",
    "                 height=5,\n",
    "                 aspect=2)\n",
    "\n",
    "ax.set(title='Grid search acccuracy', xlabel='C: Inverse of regularization strength', ylabel='Model accuracy', xscale = 'log')\n",
    "\n",
    "print('Best estimator: '+ str(classifier.best_estimator_))\n",
    "print('Parameters: ' + str(classifier.best_params_))\n",
    "print('Accuracy: ' + str(classifier.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model \n",
    "model = SVC(kernel='rbf', C=1)\n",
    "model.fit(X_train_scaled,y_train)\n",
    "# Confusion matrix\n",
    "y_predict = model.predict(X_test_scaled)\n",
    "print(metrics.classification_report(y_test, y_predict))\n",
    "cm = metrics.confusion_matrix(y_test, y_predict)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-P_G196KW2_k"
   },
   "source": [
    "Tarea 11.1 \n",
    "1. Implementar un SVM para clasificar los siguientes datasets: make_moons, make_circles y make_bloobs, para ello se deberá crear un grid search. \n",
    "2. Con los mejores párametros dibujar  las fronteras de clasificación\n",
    "3. Con los mejores parámetros dibujar la matriz de confusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a39d6xRHXsC7"
   },
   "outputs": [],
   "source": [
    "#Nota: si quiere hacer grid search con diferentes tipos de kernel se puede pedir secuencial la búsqueda por ejemplo\n",
    "param_grid = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    " ]\n",
    "#luego no podriamos graficar para comparar diferentes modelos pero podriamos pedir el mejor estimador de todos los evaluados"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sesion_11_SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
