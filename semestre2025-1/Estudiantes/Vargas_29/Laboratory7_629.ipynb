{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py"
      ],
      "metadata": {
        "id": "XS2rMa1-BteG"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Construir un clase  que permita definir una red neuronal con la topología\n",
        "deseada y la función de activación para cada capa, para ello deberá construir una funcion Topology con el número de capas de la red neuronal :\n",
        "\n",
        "Topology = [n_x, n_h1, n_h2, n_h3, ...,n_y]\n",
        "\n",
        "También definir una lista con las funciones de activaciones para cada capa.\n",
        "\n",
        "activation=[None, relu, relu, relu, ...,sigmoid]\n",
        "\n",
        "\n",
        "a. Cada unas de las capas deberá tener los parámetros de inicialización de manera aleatoria:\n",
        "\n",
        "La matriz de parametros para cada capa debera tener:\n",
        "\n",
        "\n",
        "b. Construya un metodo llamado output cuya salida serán los valores de Z y A"
      ],
      "metadata": {
        "id": "5Gt4s67R_jqv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "cd1-Pb2i9nLD"
      },
      "outputs": [],
      "source": [
        "#Función para las funciones de activacion:\n",
        "def activation_functions(name):\n",
        "\n",
        "    if name == \"Linear\":\n",
        "        return lambda x: x, lambda x: np.ones_like(x)\n",
        "\n",
        "    elif name == \"Step\":\n",
        "        return lambda x: np.where(x >= 0, 1, 0), lambda x: np.where(x >= 0, 1, 0)\n",
        "\n",
        "    elif name == \"ReLU\":\n",
        "        return lambda x: np.maximum(0, x), lambda x: (x > 0).astype(float)\n",
        "\n",
        "    elif name == \"Leaky ReLU\":\n",
        "        alpha = 0.01\n",
        "        return lambda x: np.where(x > 0, x, alpha * x), lambda x: np.where(x > 0, 1, alpha)\n",
        "\n",
        "    elif name == \"Sigmoid\":\n",
        "        return lambda x: 1 / (1 + np.exp(-x)), lambda x: (np.exp(-x)) / ((np.exp(-x) + 1) ** 2)\n",
        "\n",
        "    elif name == \"Tanh\":\n",
        "        return lambda x: np.tanh(x), lambda x: 1 - np.tanh**2\n",
        "\n",
        "    elif name == \"Softmax\":\n",
        "        return lambda x: np.exp(x) / np.sum(np.exp(x), axis=0), lambda x: np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown activation function name: {name}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clase para crear una capa\n",
        "class layer_nn():\n",
        "\n",
        "  def __init__(self,activation_function, neurons_number, before_neurons_number):\n",
        "    self.activation_function = activation_functions(activation_function)\n",
        "    self.neurons_number = neurons_number\n",
        "    self.before_neurons_number = np.zeros((before_neurons_number,1))\n",
        "    self.Theta = 2*np.random.randn(neurons_number,before_neurons_number)\n",
        "    self.B = 2*np.random.randn(neurons_number,1)\n",
        "    self.A = np.zeros((neurons_number,1))\n",
        "    #self.Z = np.zeros((neurons_number,1))\n",
        "\n",
        "  def print_parameters(self):\n",
        "    print(f\"Theta: \\n{self.Theta}\")\n",
        "    print(f\"B: \\n{self.B}\")\n",
        "\n",
        "  def output(self, X):\n",
        "    self.X = X\n",
        "    self.Z = np.dot(self.Theta,X)+self.B\n",
        "    self.A = self.activation_function[0](self.Z)\n",
        "    return self.A, self.Z, self.Theta, self.B, X\n",
        "\n",
        "  def update_parameters(self, new_Theta, new_B):\n",
        "    self.Theta = new_Theta\n",
        "    self.B = new_B\n",
        "\n",
        "  def update_A(self, new_A):\n",
        "    self.A = new_A\n",
        "\n",
        "  def get_parameters(self):\n",
        "    return self.A, self.Z, self.Theta, self.B, X"
      ],
      "metadata": {
        "id": "riz_0xRtCoDq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funcion para crear la red neuronal:\n",
        "def create_NN(topology,function_names):\n",
        "\n",
        "  if len(topology) != len(function_names):\n",
        "    raise ValueError(f\"Both lists must have the same length but: len(topology) = {len(topology)} and len(function_names) = {len(function_names)}\")\n",
        "\n",
        "  if any(x == 0 for x in topology):\n",
        "    raise ValueError(\"Each layer must have at least one neuron\")\n",
        "\n",
        "  if any(x not in [\"Linear\",\"Step\",\"ReLU\",\"Leaky ReLU\",\"Sigmoid\",\"Tanh\",\"Softmax\"] for x in function_names):\n",
        "    raise ValueError(\"Unknown activation function name\")\n",
        "\n",
        "  layers = []\n",
        "  layers.append(layer_nn(function_names[0],topology[0],1))\n",
        "\n",
        "\n",
        "  for i in range(1,len(topology)):\n",
        "    layers.append(layer_nn(function_names[i],topology[i],topology[i-1]))\n",
        "    #X = layers[i].output()[0]\n",
        "\n",
        "  return layers"
      ],
      "metadata": {
        "id": "D9PjQqgHCbMd"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Construir un generalizacion de la red, en el que entrada el valor inicial\n",
        "y la red neuronal completa arroje la salida y la actualizacion de la red con los parametros deseados:"
      ],
      "metadata": {
        "id": "IzXw_6XbqNlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(NN,A):\n",
        "\n",
        "  for layer in NN:\n",
        "    if NN.index(layer) != 0:\n",
        "      A = layer.output(A)[0]\n",
        "\n",
        "  return A"
      ],
      "metadata": {
        "id": "y7qHUGewmO_K"
      },
      "execution_count": 835,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topology = [2,4,5,4,3]\n",
        "function_names = [\"Linear\",\"ReLU\",\"Leaky ReLU\",\"ReLU\",\"Sigmoid\"]\n",
        "\n",
        "X = np.random.randn(2,1)*3\n",
        "\n",
        "NN_ = create_NN(topology,function_names)\n",
        "output = forward_pass(NN_,X)\n",
        "#np.shape(X)\n",
        "\n",
        "#output = forward_pass(NN,X)\n",
        "\n",
        "print(f\"Output: \\n{output}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "for layer in NN_:\n",
        "  print(f\"Layer {NN_.index(layer)+1}\")\n",
        "  layer.print_parameters()\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7VV4HcJOMcFb",
        "outputId": "87358688-5e50-4888-99bb-0fa0f2d8eb95"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: \n",
            "[[1.12673213e-17]\n",
            " [1.00000000e+00]\n",
            " [6.52886468e-27]]\n",
            "\n",
            "\n",
            "Layer 1\n",
            "Theta: \n",
            "[[ 1.65816394]\n",
            " [-0.51929711]]\n",
            "B: \n",
            "[[1.69900868]\n",
            " [0.56527628]]\n",
            "\n",
            "\n",
            "Layer 2\n",
            "Theta: \n",
            "[[-3.02822218  2.33422364]\n",
            " [-3.8302425  -0.04737864]\n",
            " [ 2.43322071 -0.58782382]\n",
            " [-2.48230663 -1.03735043]]\n",
            "B: \n",
            "[[-0.29919418]\n",
            " [ 0.4152287 ]\n",
            " [ 0.47187441]\n",
            " [-0.9270015 ]]\n",
            "\n",
            "\n",
            "Layer 3\n",
            "Theta: \n",
            "[[-4.39761625 -2.71944956 -3.29269981 -2.30338217]\n",
            " [ 4.74466808  2.10389434 -0.24681086 -1.2945599 ]\n",
            " [ 0.24882804 -0.50772973 -1.74650089 -1.57048046]\n",
            " [-0.43131952  0.08616077  1.3646298  -1.40754135]\n",
            " [-1.80592219 -1.13968162 -1.59853704  0.41552202]]\n",
            "B: \n",
            "[[-2.90544557]\n",
            " [ 2.95243035]\n",
            " [-2.05611498]\n",
            " [-0.01774236]\n",
            " [-3.37894871]]\n",
            "\n",
            "\n",
            "Layer 4\n",
            "Theta: \n",
            "[[-1.21516608  1.21782116  4.82132275 -1.89262424 -3.41359445]\n",
            " [ 3.52794445  2.50719191 -3.51709493  0.46328059  1.25064406]\n",
            " [ 6.12287261  0.2125848  -0.84563694 -2.67802716 -1.38076278]\n",
            " [ 2.82648188 -2.56973872  0.14146583  2.55770849  5.13941107]]\n",
            "B: \n",
            "[[ 0.26724718]\n",
            " [-2.70305175]\n",
            " [ 2.75721718]\n",
            " [ 1.62619492]]\n",
            "\n",
            "\n",
            "Layer 5\n",
            "Theta: \n",
            "[[-2.22418958 -0.62940101  0.44993313  0.00402203]\n",
            " [ 0.14023434  1.0819062  -0.5851659  -3.02291409]\n",
            " [ 0.27275052 -3.04736406 -0.07308806 -0.0568078 ]]\n",
            "B: \n",
            "[[-0.42403476]\n",
            " [-0.31396725]\n",
            " [-1.88167295]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Encontrar la funcion de coste.\n",
        "\n",
        "\n",
        "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$"
      ],
      "metadata": {
        "id": "JT8vRe5Y-LGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(y_data,y_predicted,m):\n",
        "  y_data = np.array(y_data)\n",
        "  y_predicted = np.clip(y_predicted, 1e-15, 1 - 1e-15)\n",
        "  return -(1/m)*np.sum(y_data*np.log(y_predicted)+(1-y_data)*np.log(1-y_predicted))"
      ],
      "metadata": {
        "id": "uZqQltQtBb04"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hallaremos la función coste para el dataset propuesto:"
      ],
      "metadata": {
        "id": "j1VkgbNx_mWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_train= f\"train_catvnoncat.h5\"\n",
        "train_dataset = h5py.File(data_train, \"r\")\n",
        "\n",
        "data_test= f\"test_catvnoncat.h5\"\n",
        "test_dataset = h5py.File(data_test, \"r\")\n",
        "\n",
        "xtrain_classes, xtrain, train_label =\\\n",
        "train_dataset[\"list_classes\"],train_dataset[\"train_set_x\"],train_dataset[\"train_set_y\"]\n",
        "\n",
        "test_classes, xtest,test_label =\\\n",
        "test_dataset[\"list_classes\"],test_dataset[\"test_set_x\"],test_dataset[\"test_set_y\"]\n",
        "\n",
        "xtrain_= np.reshape(xtrain,(209, 64*64*3))/255\n",
        "xtest_ = np.reshape(xtest,(50, 64*64*3))/255"
      ],
      "metadata": {
        "id": "0ezxLKJx-Dtw"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = np.shape(xtrain_)[1]\n",
        "topology = [n,4,5,4,1]\n",
        "function_names = [\"Linear\",\"ReLU\",\"Leaky ReLU\",\"ReLU\",\"Sigmoid\"]\n",
        "NN = create_NN(topology,function_names)\n",
        "output = forward_pass(NN,xtrain_.T)\n",
        "error = loss_function(train_label,output,np.shape(xtrain_)[0])\n",
        "print(\"Loss: \",error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXw94GWuAlVK",
        "outputId": "e0e7dbc8-2091-49f7-97e7-1394f6dc11ff"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:  6.079183570643172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-4ff874b71dab>:18: RuntimeWarning: overflow encountered in exp\n",
            "  return lambda x: 1 / (1 + np.exp(-x)), lambda x: (np.exp(-x)) / ((np.exp(-x) + 1) ** 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como no se ha entrenado y los pesos se asignan aleatoriamente aproximadamente la mitad debe acertar."
      ],
      "metadata": {
        "id": "STrcdfVsfvt6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Construir un codigo que permita realizar el BackwardPropagation:\n",
        "\n",
        "En el código siguiente se encuentran todas las funciones definidas previamente incluyendo la función de retropropagación y la función para entrenar la red:"
      ],
      "metadata": {
        "id": "slX5qIwEVCYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funciones:"
      ],
      "metadata": {
        "id": "qyGHiXkGi86A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Función para las funciones de activacion:\n",
        "def activation_functions(name):\n",
        "\n",
        "    if name == \"Linear\":\n",
        "        return lambda x: x, lambda x: np.ones_like(x)\n",
        "\n",
        "    elif name == \"Step\":\n",
        "        return lambda x: np.where(x >= 0, 1, 0), lambda x: np.where(x >= 0, 1, 0)\n",
        "\n",
        "    elif name == \"ReLU\":\n",
        "        return lambda x: np.maximum(0, x), lambda x: (x > 0).astype(float)\n",
        "\n",
        "    elif name == \"Leaky ReLU\":\n",
        "        alpha = 0.01\n",
        "        return lambda x: np.where(x > 0, x, alpha * x), lambda x: np.where(x > 0, 1, alpha)\n",
        "\n",
        "    elif name == \"Sigmoid\":\n",
        "        #return lambda x: 1 / (1 + np.exp(-x)), lambda x: (np.exp(-x)) / ((np.exp(-x) + 1) ** 2)\n",
        "        return lambda x: 1 / (1 + np.exp(-x)), lambda x: (1 / (1 + np.exp(-x))) * (1 - (1 / (1 + np.exp(-x))))\n",
        "    elif name == \"Tanh\":\n",
        "        return lambda x: np.tanh(x), lambda x: 1 - np.tanh(x)**2\n",
        "\n",
        "    elif name == \"Softmax\":\n",
        "        return lambda x: np.exp(x) / np.sum(np.exp(x), axis=0), lambda x: np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown activation function name: {name}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Clase para crear una capa\n",
        "class layer_nn():\n",
        "\n",
        "  def __init__(self,activation_function, neurons_number, before_neurons_number):\n",
        "    self.activation_function = activation_functions(activation_function)\n",
        "    self.neurons_number = neurons_number\n",
        "    self.before_neurons_number = np.zeros((before_neurons_number,1))\n",
        "    self.Theta = np.sqrt(2. / before_neurons_number) * np.random.randn(neurons_number,before_neurons_number)\n",
        "    self.B = np.zeros((neurons_number,1))\n",
        "    self.A = np.zeros((neurons_number,1))\n",
        "    #self.Z = np.zeros((neurons_number,1))\n",
        "\n",
        "  def print_parameters(self):\n",
        "    print(f\"Theta: \\n{self.Theta}\")\n",
        "    print(f\"B: \\n{self.B}\")\n",
        "\n",
        "  def output(self, X):\n",
        "    self.X = X\n",
        "    self.Z = np.dot(self.Theta,X)+self.B\n",
        "    self.A = self.activation_function[0](self.Z)\n",
        "    return self.A, self.Z, self.Theta, self.B, X\n",
        "\n",
        "  def update_parameters(self, new_Theta, new_B):\n",
        "    self.Theta = new_Theta\n",
        "    self.B = new_B\n",
        "\n",
        "  def update_A(self, new_A):\n",
        "    self.A = new_A\n",
        "\n",
        "  def get_parameters(self):\n",
        "    return self.A, self.Z, self.Theta, self.B, self.X\n",
        "\n",
        "\n",
        "# Funcion para crear la red neuronal:\n",
        "def create_NN(topology,function_names):\n",
        "\n",
        "  if len(topology) != len(function_names):\n",
        "    raise ValueError(f\"Both lists must have the same length but: len(topology) = {len(topology)} and len(function_names) = {len(function_names)}\")\n",
        "\n",
        "  if any(x == 0 for x in topology):\n",
        "    raise ValueError(\"Each layer must have at least one neuron\")\n",
        "\n",
        "  if any(x not in [\"Linear\",\"Step\",\"ReLU\",\"Leaky ReLU\",\"Sigmoid\",\"Tanh\",\"Softmax\"] for x in function_names):\n",
        "    raise ValueError(\"Unknown activation function name\")\n",
        "\n",
        "  layers = []\n",
        "  layers.append(layer_nn(function_names[0],topology[0],1))\n",
        "\n",
        "\n",
        "  for i in range(1,len(topology)):\n",
        "    layers.append(layer_nn(function_names[i],topology[i],topology[i-1]))\n",
        "    #X = layers[i].output()[0]\n",
        "\n",
        "  return layers\n",
        "\n",
        "\n",
        "# Propagacion hacia adelante:\n",
        "def forward_pass(NN,A):\n",
        "  for layer in NN:\n",
        "    if NN.index(layer) != 0:\n",
        "      A = layer.output(A)[0]\n",
        "  return A\n",
        "\n",
        "# Funcion de perdida de entropia cruzada:\n",
        "def loss_function(y_data,y_predicted,m):\n",
        "  y_data = np.array(y_data)\n",
        "  y_predicted = np.clip(y_predicted, 1e-15, 1 - 1e-15)\n",
        "  return -(1/m)*np.sum(y_data*np.log(y_predicted)+(1-y_data)*np.log(1-y_predicted))\n",
        "\n",
        "\n",
        "def loss_function(y_data, y_predicted, m):\n",
        "    y_data = np.array(y_data)\n",
        "    y_predicted = np.clip(y_predicted, 1e-15, 1 - 1e-15)\n",
        "    return -np.mean(y_data*np.log(y_predicted) + (1-y_data)*np.log(1-y_predicted))\n",
        "\n",
        "# Backward propagation:\n",
        "def backward_propagation(df,Theta, B, A, Z, dA_l, alpha, m):\n",
        "\n",
        "  dZ = (dA_l*df(Z))\n",
        "  #print(f\"dZ: \\n{dZ}\")\n",
        "  #print(f\"dA_l: \\n{dA_l}\")\n",
        "  #print(f\"df: \\n{df(Z)}\")\n",
        "  dTheta = 1/m * np.dot(A,dZ.T)\n",
        "  dB = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
        "  dA_1 = np.dot(Theta.T,dZ)\n",
        "\n",
        "  Theta = Theta - alpha*dTheta.T\n",
        "  B = B - alpha*dB\n",
        "\n",
        "  return Theta, B, dA_1\n",
        "\n",
        "\n",
        "# Entrenamiento:\n",
        "def train(NN,X,Y,alpha,z):\n",
        "\n",
        "  Y = np.array(Y)\n",
        "\n",
        "  for i in range(z):\n",
        "    output = forward_pass(NN,X)\n",
        "    output = np.where(output < 0.5, 0, 1)\n",
        "    #print(output[0])\n",
        "    error = loss_function(Y,output[0],np.shape(X)[1])\n",
        "    print(\"Loss: \",error)\n",
        "\n",
        "    for j in range(len(NN)-1,0,-1):\n",
        "\n",
        "      A = NN[j].get_parameters()[0]\n",
        "      Z = NN[j].get_parameters()[1]\n",
        "      Theta = NN[j].get_parameters()[2]\n",
        "      B = NN[j].get_parameters()[3]\n",
        "      df = NN[j].activation_function[1]\n",
        "\n",
        "      if j == len(NN)-1:\n",
        "        dA_l = ((Y)/(A+1e-10)) - ((1-Y)/(1-A+1e-10))\n",
        "        dA_l = - (np.divide(Y, A) - np.divide(1 - Y, 1 - A))\n",
        "\n",
        "      try:\n",
        "        A = NN[j-1].get_parameters()[0]\n",
        "      except:\n",
        "        A = X\n",
        "      NN[j].Theta, NN[j].B, dA_l = backward_propagation(df,Theta, B, A, Z, dA_l, alpha, np.shape(Y)[0])\n",
        "  return NN"
      ],
      "metadata": {
        "id": "shD6dHdIcsEC"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train= f\"train_catvnoncat.h5\"\n",
        "train_dataset = h5py.File(data_train, \"r\")\n",
        "\n",
        "data_test= f\"test_catvnoncat.h5\"\n",
        "test_dataset = h5py.File(data_test, \"r\")\n",
        "\n",
        "xtrain_classes, xtrain, train_label =\\\n",
        "train_dataset[\"list_classes\"],train_dataset[\"train_set_x\"],train_dataset[\"train_set_y\"]\n",
        "\n",
        "test_classes, xtest,test_label =\\\n",
        "test_dataset[\"list_classes\"],test_dataset[\"test_set_x\"],test_dataset[\"test_set_y\"]\n",
        "\n",
        "xtrain_= np.reshape(xtrain,(209, 64*64*3))/255\n",
        "xtrain_ = (np.reshape(xtrain, (209, 64*64*3)) - 127.5) / 127.5\n",
        "xtest_ = np.reshape(xtest,(50, 64*64*3))/255\n",
        "xtest_ = (np.reshape(xtest, (50, 64*64*3)) - 127.5) / 127.5\n",
        "\n",
        "n = np.shape(xtrain_)[1]\n",
        "topology = [n,4,5,4,1]\n",
        "function_names = [\"Linear\",\"Tanh\",\"Tanh\",\"Tanh\",\"Sigmoid\"]\n",
        "NN = create_NN(topology,function_names)\n",
        "output = forward_pass(NN,xtrain_.T)\n",
        "error = loss_function(train_label,output,np.shape(xtrain_)[0])\n",
        "print(\"Loss: \",error)\n",
        "train(NN,xtrain_.T,train_label,0.01,1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Gdz8efY4jzSI",
        "outputId": "db388fe5-ba74-4e6d-9154-874296430a31"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:  0.7818371521150569\n",
            "Loss:  16.52599431441986\n",
            "Loss:  14.046981731544898\n",
            "Loss:  13.551213647395128\n",
            "Loss:  11.568114530020875\n",
            "Loss:  12.39438956917046\n",
            "Loss:  11.23762287832116\n",
            "Loss:  11.898610007545615\n",
            "Loss:  10.411324884221427\n",
            "Loss:  11.733352704220684\n",
            "Loss:  9.254512283471835\n",
            "Loss:  10.741793580970993\n",
            "Loss:  9.089251154321877\n",
            "Loss:  10.080756716021218\n",
            "Loss:  8.758732721846991\n",
            "Loss:  8.923944115271624\n",
            "Loss:  7.43664368864734\n",
            "Loss:  7.932400295322034\n",
            "Loss:  7.106125256172453\n",
            "Loss:  7.601881862847146\n",
            "Loss:  6.94087560449757\n",
            "Loss:  7.601885688672171\n",
            "Loss:  6.775618301172639\n",
            "Loss:  8.262914901971898\n",
            "Loss:  6.775622126997664\n",
            "Loss:  8.262911076146873\n",
            "Loss:  6.940887081972646\n",
            "Loss:  8.262903424496823\n",
            "Loss:  6.610372475322782\n",
            "Loss:  7.436613082047141\n",
            "Loss:  5.784066829573001\n",
            "Loss:  5.949297352122758\n",
            "Loss:  5.123022312973175\n",
            "Loss:  5.949301177947783\n",
            "Loss:  4.957765009648244\n",
            "Loss:  5.618778919647871\n",
            "Loss:  4.461981622198374\n",
            "Loss:  5.1229993580230255\n",
            "Loss:  4.627242751348331\n",
            "Loss:  4.957738228873069\n",
            "Loss:  4.461981622198374\n",
            "Loss:  4.957738228873069\n",
            "Loss:  4.296720493048418\n",
            "Loss:  4.792480925548138\n",
            "Loss:  3.9662020605735306\n",
            "Loss:  4.627223622223206\n",
            "Loss:  3.800940931423574\n",
            "Loss:  4.461966318898275\n",
            "Loss:  3.800940931423574\n",
            "Loss:  4.461966318898275\n",
            "Loss:  3.6356836280986427\n",
            "Loss:  4.296705189748319\n",
            "Loss:  3.4704263247737113\n",
            "Loss:  4.131444060598363\n",
            "Loss:  3.4704263247737113\n",
            "Loss:  4.131444060598363\n",
            "Loss:  3.4704263247737113\n",
            "Loss:  3.966186757273431\n",
            "Loss:  3.4704263247737113\n",
            "Loss:  3.635672150623568\n",
            "Loss:  3.305165195623755\n",
            "Loss:  3.305157543973705\n",
            "Loss:  3.305165195623755\n",
            "Loss:  3.1399002406487737\n",
            "Loss:  3.1399040664737985\n",
            "Loss:  2.974642937323842\n",
            "Loss:  3.1399040664737985\n",
            "Loss:  2.974642937323842\n",
            "Loss:  3.1399040664737985\n",
            "Loss:  2.8093856339989105\n",
            "Loss:  2.9746467631488667\n",
            "Loss:  2.8093856339989105\n",
            "Loss:  2.9746467631488667\n",
            "Loss:  2.8093856339989105\n",
            "Loss:  2.9746467631488667\n",
            "Loss:  2.8093856339989105\n",
            "Loss:  2.9746467631488667\n",
            "Loss:  2.8093856339989105\n",
            "Loss:  2.9746467631488667\n",
            "Loss:  2.644128330673979\n",
            "Loss:  2.4788710273490477\n",
            "Loss:  2.4788710273490477\n",
            "Loss:  2.4788710273490477\n",
            "Loss:  2.4788710273490477\n",
            "Loss:  2.4788710273490477\n",
            "Loss:  2.313609898199091\n",
            "Loss:  2.4788710273490477\n",
            "Loss:  2.1483525948741597\n",
            "Loss:  2.4788710273490477\n",
            "Loss:  2.313609898199091\n",
            "Loss:  2.4788710273490477\n",
            "Loss:  2.313609898199091\n",
            "Loss:  2.4788710273490477\n",
            "Loss:  2.313609898199091\n",
            "Loss:  2.313609898199091\n",
            "Loss:  2.1483525948741597\n",
            "Loss:  2.313609898199091\n",
            "Loss:  2.1483525948741597\n",
            "Loss:  2.313609898199091\n",
            "Loss:  2.1483525948741597\n",
            "Loss:  2.1483525948741597\n",
            "Loss:  2.1483525948741597\n",
            "Loss:  1.9830952915492286\n",
            "Loss:  2.1483525948741597\n",
            "Loss:  1.9830952915492286\n",
            "Loss:  2.1483525948741597\n",
            "Loss:  1.8178379882242972\n",
            "Loss:  2.1483525948741597\n",
            "Loss:  1.8178379882242972\n",
            "Loss:  2.1483525948741597\n",
            "Loss:  1.8178379882242972\n",
            "Loss:  1.9830952915492286\n",
            "Loss:  1.6525806848993658\n",
            "Loss:  1.9830952915492286\n",
            "Loss:  1.4873233815744342\n",
            "Loss:  1.9830952915492286\n",
            "Loss:  1.4873233815744342\n",
            "Loss:  1.9830952915492286\n",
            "Loss:  1.4873233815744342\n",
            "Loss:  2.1483525948741597\n",
            "Loss:  1.6525806848993658\n",
            "Loss:  1.9830952915492286\n",
            "Loss:  1.4873233815744342\n",
            "Loss:  1.9830952915492286\n",
            "Loss:  1.4873233815744342\n",
            "Loss:  1.9830952915492286\n",
            "Loss:  1.4873233815744342\n",
            "Loss:  1.9830952915492286\n",
            "Loss:  1.3220660782495024\n",
            "Loss:  1.9830952915492286\n",
            "Loss:  1.3220660782495024\n",
            "Loss:  1.9830952915492286\n",
            "Loss:  1.3220660782495024\n",
            "Loss:  1.9830952915492286\n",
            "Loss:  1.3220660782495024\n",
            "Loss:  1.9830952915492286\n",
            "Loss:  1.3220660782495024\n",
            "Loss:  1.9830952915492286\n",
            "Loss:  1.3220660782495024\n",
            "Loss:  1.9830952915492286\n",
            "Loss:  1.156808774924571\n",
            "Loss:  1.9830952915492286\n",
            "Loss:  1.156808774924571\n",
            "Loss:  1.8178379882242972\n",
            "Loss:  1.156808774924571\n",
            "Loss:  1.8178379882242972\n",
            "Loss:  1.156808774924571\n",
            "Loss:  1.8178379882242972\n",
            "Loss:  1.156808774924571\n",
            "Loss:  1.8178379882242972\n",
            "Loss:  0.9915514715996396\n",
            "Loss:  1.8178379882242972\n",
            "Loss:  0.9915514715996396\n",
            "Loss:  1.6525806848993654\n",
            "Loss:  0.9915514715996396\n",
            "Loss:  1.3220622524244776\n",
            "Loss:  0.9915514715996396\n",
            "Loss:  1.3220622524244776\n",
            "Loss:  0.9915514715996396\n",
            "Loss:  1.1568049490995462\n",
            "Loss:  0.9915514715996396\n",
            "Loss:  1.1568049490995462\n",
            "Loss:  0.9915514715996396\n",
            "Loss:  1.1568049490995462\n",
            "Loss:  0.9915514715996396\n",
            "Loss:  1.1568049490995462\n",
            "Loss:  0.8262903424496832\n",
            "Loss:  0.8262903424496832\n",
            "Loss:  0.8262903424496832\n",
            "Loss:  0.8262903424496832\n",
            "Loss:  0.8262903424496832\n",
            "Loss:  0.8262903424496832\n",
            "Loss:  0.6610330391247518\n",
            "Loss:  0.6610330391247518\n",
            "Loss:  0.6610330391247518\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.49577573579982037\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.330514606649864\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  0.16525730332493252\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n",
            "Loss:  9.992007221626413e-16\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<__main__.layer_nn at 0x79a4604e4050>,\n",
              " <__main__.layer_nn at 0x79a456ee5910>,\n",
              " <__main__.layer_nn at 0x79a45438a3d0>,\n",
              " <__main__.layer_nn at 0x79a454208390>,\n",
              " <__main__.layer_nn at 0x79a45420ab90>]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación se restarán los datos etiquetados con los predichos, si el valor predicho es correcto debe salir un cero."
      ],
      "metadata": {
        "id": "y5hFqfBWs1_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = forward_pass(NN,xtest_.T)\n",
        "output = np.where(output < 0.5, 0, 1)\n",
        "output-test_label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLXqL4hqkBHE",
        "outputId": "ac8b8517-d826-433d-da4b-8bf4d01b4c52"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  0,  0, -1,  0,  0,  0, -1, -1,  0,  1,  0,  0,\n",
              "         0,  0, -1, -1,  0,  0,  0,  0, -1,  0,  0,  0, -1,  1, -1,  0,\n",
              "         0, -1,  1,  0,  0,  0,  0,  0,  0, -1,  0,  0,  1,  1, -1,  0,\n",
              "         0,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = forward_pass(NN,xtrain_.T)\n",
        "output = np.where(output < 0.5, 0, 1)\n",
        "output-train_label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mGvvKjvoVOw",
        "outputId": "df1f4f7c-bc24-4a83-99d9-8a578f1f74d5"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí vemos que el entrenamiento hizo que la red se sobreajustara. Funciona muy bien con los datos de entrenamiento pero con los datos de testeo funciona casi igual que por azar. Intentemos disminuir los ciclos y aumentar la taza de aprendizaje."
      ],
      "metadata": {
        "id": "AAzlt5ceou7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = np.shape(xtrain_)[1]\n",
        "topology = [n,4,5,4,1]\n",
        "function_names = [\"Linear\",\"Tanh\",\"Tanh\",\"Tanh\",\"Sigmoid\"]\n",
        "NN_2 = create_NN(topology,function_names)\n",
        "output = forward_pass(NN_2,xtrain_.T)\n",
        "error = loss_function(train_label,output,np.shape(xtrain_)[0])\n",
        "print(\"Loss: \",error)\n",
        "train(NN_2,xtrain_.T,train_label,0.1,200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Mh0fUF8HopmG",
        "outputId": "421ec033-135b-405e-9d95-45ae784e28a3"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:  0.7086825508232768\n",
            "Loss:  17.682833695944634\n",
            "Loss:  12.229097833420305\n",
            "Loss:  11.568053316820478\n",
            "Loss:  11.402792187670522\n",
            "Loss:  11.237534884345589\n",
            "Loss:  11.072277581020659\n",
            "Loss:  11.072277581020659\n",
            "Loss:  10.741762974370797\n",
            "Loss:  10.741762974370797\n",
            "Loss:  10.576505671045863\n",
            "Loss:  10.576505671045863\n",
            "Loss:  10.576505671045863\n",
            "Loss:  10.576505671045863\n",
            "Loss:  10.576505671045863\n",
            "Loss:  10.245979586920926\n",
            "Loss:  9.750207676946133\n",
            "Loss:  9.915468806096088\n",
            "Loss:  9.915464980271064\n",
            "Loss:  9.915464980271064\n",
            "Loss:  9.419693070296269\n",
            "Loss:  9.5849503736212\n",
            "Loss:  9.419693070296269\n",
            "Loss:  9.089186115296457\n",
            "Loss:  8.758663856996543\n",
            "Loss:  9.089178463646407\n",
            "Loss:  8.42814925034668\n",
            "Loss:  8.923921160321475\n",
            "Loss:  7.601858907896998\n",
            "Loss:  8.758660031171518\n",
            "Loss:  14.708141022895468\n",
            "Loss:  9.089178463646407\n",
            "Loss:  8.758663856996543\n",
            "Loss:  8.428145424521656\n",
            "Loss:  8.097630817871792\n",
            "Loss:  5.949285874647683\n",
            "Loss:  7.106086997922203\n",
            "Loss:  13.881885112871009\n",
            "Loss:  9.089178463646407\n",
            "Loss:  8.593402727846588\n",
            "Loss:  7.601862733722023\n",
            "Loss:  8.097634643696818\n",
            "Loss:  6.940825868772247\n",
            "Loss:  7.601855082071973\n",
            "Loss:  8.923970896046798\n",
            "Loss:  6.279796655472522\n",
            "Loss:  6.279792829647497\n",
            "Loss:  5.6187636163477706\n",
            "Loss:  4.627219796398181\n",
            "Loss:  6.775572391272341\n",
            "Loss:  9.91561801327206\n",
            "Loss:  6.610315087947409\n",
            "Loss:  6.940829694597272\n",
            "Loss:  5.122995532198002\n",
            "Loss:  4.461966318898275\n",
            "Loss:  4.131451712248412\n",
            "Loss:  4.296709015573343\n",
            "Loss:  5.78407448122305\n",
            "Loss:  4.627219796398181\n",
            "Loss:  3.966190583098456\n",
            "Loss:  8.593536631722458\n",
            "Loss:  6.6103112621223845\n",
            "Loss:  6.114539352147591\n",
            "Loss:  6.114554655447689\n",
            "Loss:  5.288249009697908\n",
            "Loss:  4.792477099723113\n",
            "Loss:  4.461973970548325\n",
            "Loss:  3.8009409314235745\n",
            "Loss:  4.627242751348331\n",
            "Loss:  3.4704224989486865\n",
            "Loss:  2.4788786789990973\n",
            "Loss:  3.1399002406487733\n",
            "Loss:  8.758813064172513\n",
            "Loss:  4.46196249307325\n",
            "Loss:  4.296705189748319\n",
            "Loss:  4.627254228823405\n",
            "Loss:  2.8093894598239357\n",
            "Loss:  2.974650588973892\n",
            "Loss:  2.3136060723740663\n",
            "Loss:  2.1483640723492345\n",
            "Loss:  5.9492897004727086\n",
            "Loss:  10.24615557487207\n",
            "Loss:  9.419712199421394\n",
            "Loss:  6.775576217097365\n",
            "Loss:  4.792484751373163\n",
            "Loss:  2.8093818081738857\n",
            "Loss:  1.9831067690243032\n",
            "Loss:  5.123003183848052\n",
            "Loss:  16.360825005070506\n",
            "Loss:  5.288249009697908\n",
            "Loss:  4.957734403048045\n",
            "Loss:  4.46196249307325\n",
            "Loss:  2.974639111498817\n",
            "Loss:  2.644143633974079\n",
            "Loss:  2.4788633756989977\n",
            "Loss:  1.9830952915492286\n",
            "Loss:  1.4873195557494092\n",
            "Loss:  2.14834494322411\n",
            "Loss:  2.478893982299197\n",
            "Loss:  4.46196249307325\n",
            "Loss:  3.635675976448593\n",
            "Loss:  2.4788786789990978\n",
            "Loss:  1.9830876398991786\n",
            "Loss:  1.8178341623992722\n",
            "Loss:  1.3220622524244778\n",
            "Loss:  0.9915438199495898\n",
            "Loss:  1.3220584265994528\n",
            "Loss:  6.775679514373038\n",
            "Loss:  4.131447886423387\n",
            "Loss:  2.1483449432241097\n",
            "Loss:  1.3220622524244778\n",
            "Loss:  1.4873157299243842\n",
            "Loss:  4.6272848354236045\n",
            "Loss:  4.46196249307325\n",
            "Loss:  3.8009294539484992\n",
            "Loss:  1.8178341623992722\n",
            "Loss:  2.644120679023929\n",
            "Loss:  13.386120854546267\n",
            "Loss:  6.775583868747415\n",
            "Loss:  5.78407448122305\n",
            "Loss:  5.949282048822658\n",
            "Loss:  4.4619969254984735\n",
            "Loss:  6.445065436272527\n",
            "Loss:  2.4788710273490477\n",
            "Loss:  3.1399231955989224\n",
            "Loss:  3.966190583098456\n",
            "Loss:  3.139927021423947\n",
            "Loss:  3.9661944089234806\n",
            "Loss:  1.4873195557494092\n",
            "Loss:  1.8178418140493218\n",
            "Loss:  1.817864768999471\n",
            "Loss:  1.8178456398743468\n",
            "Loss:  5.453601958648461\n",
            "Loss:  5.122999358023025\n",
            "Loss:  14.708229016871039\n",
            "Loss:  4.131455538073437\n",
            "Loss:  3.1399002406487733\n",
            "Loss:  2.6441436339740783\n",
            "Loss:  4.95773822887307\n",
            "Loss:  5.618843958673292\n",
            "Loss:  5.949285874647683\n",
            "Loss:  3.139896414823749\n",
            "Loss:  2.8094085889490596\n",
            "Loss:  4.46196249307325\n",
            "Loss:  2.148348769049135\n",
            "Loss:  3.3051881505739042\n",
            "Loss:  3.6356721506235683\n",
            "Loss:  1.9830991173742532\n",
            "Loss:  1.6525883365494152\n",
            "Loss:  1.4873195557494092\n",
            "Loss:  1.3220622524244778\n",
            "Loss:  1.3220622524244778\n",
            "Loss:  1.1568049490995462\n",
            "Loss:  1.1568049490995462\n",
            "Loss:  1.1568049490995462\n",
            "Loss:  1.1568049490995462\n",
            "Loss:  0.9915438199495898\n",
            "Loss:  0.9915438199495898\n",
            "Loss:  0.9915438199495898\n",
            "Loss:  0.9915438199495898\n",
            "Loss:  0.9915438199495898\n",
            "Loss:  0.9915438199495898\n",
            "Loss:  0.9915438199495898\n",
            "Loss:  0.9915438199495898\n",
            "Loss:  0.9915438199495898\n",
            "Loss:  0.8262865166246585\n",
            "Loss:  0.8262865166246585\n",
            "Loss:  0.8262865166246585\n",
            "Loss:  0.8262865166246585\n",
            "Loss:  0.8262865166246585\n",
            "Loss:  0.8262865166246585\n",
            "Loss:  0.8262865166246585\n",
            "Loss:  0.8262865166246585\n",
            "Loss:  0.8262865166246585\n",
            "Loss:  0.8262865166246585\n",
            "Loss:  0.8262865166246585\n",
            "Loss:  0.8262865166246585\n",
            "Loss:  0.8262865166246585\n",
            "Loss:  0.8262865166246585\n",
            "Loss:  0.8262865166246585\n",
            "Loss:  0.8262865166246585\n",
            "Loss:  0.8262865166246585\n",
            "Loss:  0.6610292132997269\n",
            "Loss:  0.6610292132997269\n",
            "Loss:  0.6610292132997269\n",
            "Loss:  0.6610292132997269\n",
            "Loss:  0.6610292132997269\n",
            "Loss:  0.6610292132997269\n",
            "Loss:  0.6610292132997269\n",
            "Loss:  0.6610292132997269\n",
            "Loss:  0.6610292132997269\n",
            "Loss:  0.6610292132997269\n",
            "Loss:  0.6610292132997269\n",
            "Loss:  0.6610292132997269\n",
            "Loss:  0.6610292132997269\n",
            "Loss:  0.6610292132997269\n",
            "Loss:  0.6610292132997269\n",
            "Loss:  0.6610292132997269\n",
            "Loss:  0.6610292132997269\n",
            "Loss:  0.6610292132997269\n",
            "Loss:  0.6610292132997269\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<__main__.layer_nn at 0x79a45420a290>,\n",
              " <__main__.layer_nn at 0x79a45420a0d0>,\n",
              " <__main__.layer_nn at 0x79a46030b490>,\n",
              " <__main__.layer_nn at 0x79a47af13490>,\n",
              " <__main__.layer_nn at 0x79a461b19c90>]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = forward_pass(NN_2,xtest_.T)\n",
        "output = np.where(output < 0.5, 0, 1)\n",
        "output-test_label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx5edxjvpI3e",
        "outputId": "606fa2d4-6d29-4267-a2a9-64272d8d373d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0, -1,  0, -1,  0,  0, -1, -1, -1,  0,  1,  0, -1,\n",
              "         0,  0, -1, -1,  0,  0,  0,  0,  0, -1,  0,  0, -1,  1, -1,  0,\n",
              "         0, -1,  1,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0, -1,  0,\n",
              "         0,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = forward_pass(NN_2,xtrain_.T)\n",
        "output = np.where(output < 0.5, 0, 1)\n",
        "output-train_label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdmkdyQspQ0S",
        "outputId": "236f4e59-c59f-4083-a1c4-b26ffea55a84"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0]])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hay una leve mejora con los datos de testeo."
      ],
      "metadata": {
        "id": "wftrJI_GrYba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = np.shape(xtrain_)[1]\n",
        "topology = [n,4,5,4,1]\n",
        "function_names = [\"Linear\",\"Tanh\",\"Tanh\",\"Tanh\",\"Sigmoid\"]\n",
        "NN_3 = create_NN(topology,function_names)\n",
        "output = forward_pass(NN_3,xtrain_.T)\n",
        "error = loss_function(train_label,output,np.shape(xtrain_)[0])\n",
        "print(\"Loss: \",error)\n",
        "train(NN_3,xtrain_.T,train_label,0.1,50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mOVfde5upf1B",
        "outputId": "63cb9c06-a14b-42f1-afca-0d5de7d08caa"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:  0.7528301849582856\n",
            "Loss:  21.483847318043676\n",
            "Loss:  13.220660782495017\n",
            "Loss:  12.559627743370266\n",
            "Loss:  12.063844355920398\n",
            "Loss:  11.568060968470528\n",
            "Loss:  11.402799839320572\n",
            "Loss:  11.402796013495546\n",
            "Loss:  11.072281406845685\n",
            "Loss:  10.907020277695725\n",
            "Loss:  10.741762974370797\n",
            "Loss:  10.411244541895908\n",
            "Loss:  10.245987238570976\n",
            "Loss:  10.080729935246044\n",
            "Loss:  9.750215328596182\n",
            "Loss:  9.419693070296269\n",
            "Loss:  9.254424289496264\n",
            "Loss:  9.584942721971151\n",
            "Loss:  8.593383598721463\n",
            "Loss:  9.089166986171332\n",
            "Loss:  9.254424289496264\n",
            "Loss:  7.932354385421736\n",
            "Loss:  8.097611688746667\n",
            "Loss:  8.097615514571693\n",
            "Loss:  7.601839778771874\n",
            "Loss:  8.923913508671426\n",
            "Loss:  8.593402727846588\n",
            "Loss:  7.932358211246761\n",
            "Loss:  7.932354385421736\n",
            "Loss:  8.262880469546674\n",
            "Loss:  7.601855082071973\n",
            "Loss:  6.6102921329972615\n",
            "Loss:  6.940814391297173\n",
            "Loss:  7.271328997947036\n",
            "Loss:  7.1060755204471295\n",
            "Loss:  7.767154469472178\n",
            "Loss:  7.4365863012719675\n",
            "Loss:  6.44503482967233\n",
            "Loss:  7.271336649597085\n",
            "Loss:  6.775595346222489\n",
            "Loss:  6.11453170049754\n",
            "Loss:  4.95772292557297\n",
            "Loss:  3.9662020605735306\n",
            "Loss:  3.4704186731236613\n",
            "Loss:  8.097634643696818\n",
            "Loss:  13.716623983721053\n",
            "Loss:  4.462000751323498\n",
            "Loss:  6.775560913797267\n",
            "Loss:  4.627204493098082\n",
            "Loss:  4.2966898864482195\n",
            "Loss:  2.64412067902393\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<__main__.layer_nn at 0x79a4541f7b10>,\n",
              " <__main__.layer_nn at 0x79a454208ed0>,\n",
              " <__main__.layer_nn at 0x79a45438add0>,\n",
              " <__main__.layer_nn at 0x79a4541f7790>,\n",
              " <__main__.layer_nn at 0x79a4541f7810>]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = forward_pass(NN_3,xtest_.T)\n",
        "output = np.where(output < 0.5, 0, 1)\n",
        "output-test_label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHQWE6Iori-z",
        "outputId": "aa64b311-5c33-4cb6-87d5-83439239d045"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0, -1, -1,  0, -1,  0,  0, -1,  0, -1,  0,  0,  0,  0,\n",
              "         0,  0, -1, -1,  0,  0,  0, -1,  0,  0, -1,  0, -1,  1, -1, -1,\n",
              "         0, -1,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0, -1,  0,\n",
              "        -1,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = forward_pass(NN_3,xtrain_.T)\n",
        "output = np.where(output < 0.5, 0, 1)\n",
        "output-train_label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZxf9wBorssW",
        "outputId": "2f405b20-4f30-4183-f333-0b6782953312"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0, -1,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0, -1,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  1,\n",
              "         0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  1,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0]])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fy1T-yDurxlM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}