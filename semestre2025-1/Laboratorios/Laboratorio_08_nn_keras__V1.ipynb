{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/CienciaDatosUdea/002_EstudiantesAprendizajeEstadistico/blob/main/semestre2025-1/Laboratorios/Laboratorio_08_nn_keras__V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Función de activación | Ventajas | Desventajas |\n",
    "| --------------------- | -------- | ----------- |\n",
    "| Lineal | Simple y rápida de calcular | Le falta introducir no linealidad, no limita el rango de salida |\n",
    "| Sigmoide | Mapea la salida al rango (0, 1), útil para clasificación binaria | Se satura fácilmente, tiene gradientes pequeños, no tiene media cero |\n",
    "| Tangente hiperbólica | Mapea la salida al rango (-1, 1), tiene media cero, es más fuerte que la sigmoide | Se satura fácilmente, tiene gradientes pequeños |\n",
    "| ReLU | Introduce no linealidad, tiene gradientes grandes, es rápida de calcular | Puede morir si la entrada es negativa, no tiene media cero |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8yDwsbFN4sJ"
   },
   "source": [
    "# Keras: https://keras.io/api/layers/activations/\n",
    "\n",
    "Keras es una bilbioteca con librerias de código abierto para entrenar modelos de deep learning DL que puede ejecutarse en TensorFlow. Keras permite construir bloque de arquitectura de la redes neuronales, y permite construir desde un perceptron hasta redes neuronales convolucionales  y recurrentes de una manera amigable con el usuario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vFY3rPvQVK0"
   },
   "source": [
    "# Funciones de activación\n",
    "\n",
    "Tipos de funciones de activación:\n",
    "\n",
    "## Lineal:\n",
    "\n",
    "Una transformación lineal, la cual se muestra en la siguiente figura, consiste básicamente, en la función identidad. En estra transformación, la variable dependiente tiene una relación directa y proporcional con la variable independiente. En términos prácticos, lo anterior significa que una función de activación lineal pasa la señal sin realizar un cambio sobre esta. Este el típo de activación que se usa en la capa de entrada de las redes neuronales.\n",
    "\n",
    "\n",
    "## Sigmoide:\n",
    "Como todas las transformaciones logísticas, las sigmoides puede reducir la cantidad de valores extremos o outliers en los datos sin eliminarlos. En la siguiente figura se ilustra dicha función sigmoide:\n",
    "\n",
    "Una función sigmoide convierde una variable independiente de rango infinito en probabilidades con un rango entre 0 y 1. La mayoria de las salidas serán cercanas a 0 o 1, que corresponden a zonas de saturación.\n",
    "\n",
    "La función de activación sigmoide devuelve una probabilidad independiente para cada clase.\n",
    "\n",
    "\n",
    "\n",
    "##  Tanh\n",
    "Corresponde a una función trigonométrica hiperbólica donde el rango de salida de tanh varía entre -1 y 1, note la diferencia con la función sigmoide. La ventaja de tanh es puede tratar los números negativos de forma más eficiente que la función sigmoide.\n",
    "\n",
    "\n",
    "## ReLU: Rectified Linear Unit (Unidad Rectificadad Lineal)\n",
    "La unidad rectificada lineal es una trasformación más intersante que activa un nodo solo si la entrada se encuentra por encima de cierta cantidad. Si la entrada está por debajo de zero, la salida es cero, pero si la entrada alcanza un valor umbral, la salida tiene una relación lineal con la variable dependiente.\n",
    "\n",
    "Las funciones de activaciones ReLU representan actualmente el estado de arte en la aplicación de funciones de activación, ya que han demostrado un buen desempeño en diferentes situaciones. Dado que el gradiente de la ReLU es cero o constante, permite evitar el problema del desvanecimiento del gradiente. Las funciones de activación ReLu han demostrado un mejor entrenamiento en la práctica que las funciones de activación sigmoidea.\n",
    "\n",
    "Las funciones de activación ReLU se usan, normalmente, en las capas ocultas.\n",
    "\n",
    "Existen mas funciones de activación, no obstante, nos centraremos a continuación solo en una más. Esta función de activación es usada en la capa de salida de la clasificación multiclase mediante una red neuronal. Es conocida como Softmax.\n",
    "\n",
    "\n",
    "## Softmax\n",
    "La función de activación Softmax representa una generalización de la regresión logística en el sentido que puede ser apllicada a un conjunto continuos de datos (en lugar de una clasificación binaria) y puede contener múltiples fronteras de desición. Esta función maneja sistemas multinomiales para el etiquetado. Softmax es una función que normalmente se usa en la capa de salida de un clasificador.\n",
    "\n",
    "La función de activación Softmax devuelve la distribución de probablilidades de clases mutuamente excluyentes.\n",
    "\n",
    "Consideremos el siguiente ejemplo en donde se ilustra, entre otra cosas la definición de la función SoftMax:\n",
    "\n",
    "$S(y_i)=\\frac{e^y_i}{\\sum e^{y_i}}$\n",
    "\n",
    "\n",
    "En deep learning, el térno capa logit se usa para la última capa de neuronas de la red neuronal para el problema de clasificación que produce unos valores de predicción \"crudos\" de valores reales que varian en el intervalo . En concreto, las cantidades logits son los puntajes crudos de la ultima capa de la red neuronal, antes de que se les aplique la activación.\n",
    "\n",
    "\n",
    "Ref: https://github.com/ssanchezgoe/curso_deep_learning_economia/blob/main/NBs_Google_Colab/DL_S07_Funciones_Activacion_Arquitectura_Red.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VqNJ24H2OqJa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PAGKdoa6Pe1b",
    "outputId": "775c7477-4da3-460f-ef7a-c28e2e275416"
   },
   "outputs": [],
   "source": [
    "# 1: A través de una capa medidante el método Activación:\n",
    "modelA = keras.models.Sequential()\n",
    "modelA.add(keras.layers.Dense(32, input_shape=(16,)))\n",
    "modelA.add(keras.layers.Activation('relu'))#  Capa activación\n",
    "modelA.add(keras.layers.Dense(64))\n",
    "modelA.add(keras.layers.Activation('tanh'))#  Capa activación\n",
    "modelA.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M12mxXtuPon5",
    "outputId": "10da03c4-4379-41a2-8bed-6a38cba0bc05"
   },
   "outputs": [],
   "source": [
    "# 2: Mediante el argumento activation\n",
    "modelB = keras.models.Sequential()\n",
    "modelB.add(keras.layers.Dense(32, input_shape=(16,)))\n",
    "modelA.add(keras.layers.Activation('relu'))#  Capa activación\n",
    "modelB.add(keras.layers.Dense(64, activation='tanh'))\n",
    "modelB.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a26fmJ4lP3qB",
    "outputId": "a1afcc49-603d-44cb-f8b6-c0abc65fb071"
   },
   "outputs": [],
   "source": [
    "# 3: Mediante una función de Tensor-flow por elementos.\n",
    "modelC = keras.models.Sequential()\n",
    "modelC.add(keras.layers.Dense(32, activation='relu', input_shape=(16,)))\n",
    "modelC.add(keras.layers.Dense(64, activation='tanh'))\n",
    "modelC.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "SPcwCV5FQOzQ",
    "outputId": "904711a4-2783-4e9c-d879-acfe9160e753"
   },
   "outputs": [],
   "source": [
    "# Vamos a graficar las funciones de activación y modificar los parámetros que las definen\n",
    "# Encuentran los parámetros en https://keras.io/api/layers/activations/\n",
    "# Relu\n",
    "import matplotlib.pyplot as plt\n",
    "x=np.random.uniform(low=-10.0, high=10, size=(2000,))\n",
    "y=keras.activations.relu(x, negative_slope=0, max_value=None, threshold=0)\n",
    "plt.plot(x,y,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "8wzaac3ISyKK",
    "outputId": "81859ad9-f55b-441b-857d-d2afb52a0c44"
   },
   "outputs": [],
   "source": [
    "# tanh\n",
    "y=keras.activations.tanh(x)\n",
    "plt.plot(x,y,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "Hpi1LJHjS0jb",
    "outputId": "515878ef-57fe-424d-cda4-d648e9ba0d7c"
   },
   "outputs": [],
   "source": [
    "#sigmoid\n",
    "y=keras.activations.sigmoid(x)\n",
    "plt.plot(x,y,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "5buNZl9zS3Wb",
    "outputId": "93a8c173-4bf8-4472-8bcc-4a0b4b1a88b0"
   },
   "outputs": [],
   "source": [
    "# elu\n",
    "y=keras.activations.elu(x, alpha=1.0)\n",
    "plt.plot(x,y,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "-NzZTPBiS4Op",
    "outputId": "b241176c-c7e3-40c8-d5e7-871302c9af38"
   },
   "outputs": [],
   "source": [
    "# Exponential\n",
    "y=keras.activations.exponential(x)\n",
    "plt.plot(x,y,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "78UwhBnvS7Mr",
    "outputId": "0477027a-8de5-4975-d0a8-038559914ca8"
   },
   "outputs": [],
   "source": [
    "# Selu\n",
    "y=keras.activations.selu(x)\n",
    "plt.plot(x,y,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "djOhkiMnS9lP",
    "outputId": "2e2d4d21-4e15-4ef3-c72c-7d6750148d7d"
   },
   "outputs": [],
   "source": [
    "# Linear\n",
    "y=keras.activations.linear(x)\n",
    "plt.plot(x,y,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "L7duyMfES_ge",
    "outputId": "52526699-1319-4c38-d338-0de21f87dcb7"
   },
   "outputs": [],
   "source": [
    "y=keras.activations.softplus(x)\n",
    "plt.plot(x,y,'.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "4xKc4uh0SfcI",
    "outputId": "fc1b5395-abb4-47aa-d3d0-3b862453324f"
   },
   "outputs": [],
   "source": [
    "y=keras.activations.softsign(x)\n",
    "plt.plot(x,y,'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbUbsVc5TRjZ"
   },
   "source": [
    "# Arquitectura y funcionalidad de la Redes neuronales secuenciales:\n",
    "\n",
    "De las funciones de activación habladas en la clase anterior podemos advertir dos características que deben poseer una red neuronal:\n",
    "\n",
    "Las funciones de activación de las capas ocultas deben ser funciones de activación no lineales, con el fín de que la red actue como un aproximador universal a una función.\n",
    "\n",
    "La función de activación de la capa de salida determina el tipo de clasificación/regresión del problema que se pretende solucionar.\n",
    "Como regla general, se tiene que la función de activación de las capas ocultas puede ser definida como una función ReLU y, dependiendo del problema, podemos definir la función de activación de la capa de salida como:\n",
    "\n",
    "- Función de activación sigmoide: si el problema de clasificación es binario.\n",
    "- Función de activación Softmax: si el problema de clasificación es multiclase.\n",
    "- Función de activación lineal: si el problema se trata de una regresión.\n",
    "\n",
    "En resumen, en la siguiente figura se ilustran la arquitectura de red de los problemas que pueden presentarse en la clasificación/regresión usando una red neuronal secuencial y las funciones de activación definidas en las capas que la componen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDhBLuL5Sv1V"
   },
   "outputs": [],
   "source": [
    "# https://github.com/ssanchezgoe/curso_deep_learning_economia/blob/main/NBs_Google_Colab/DL_S08_Problemas_FFNN.ipynb\n",
    "# https://github.com/ssanchezgoe/curso_deep_learning_economia/blob/main/NBs_Google_Colab/DL_S11_DNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "8COpdYhhWm3T",
    "outputId": "14673ecb-dc25-4af1-9db1-2085894e9c7b"
   },
   "outputs": [],
   "source": [
    "# generación de círculos\n",
    "# Algunas librerías necesarias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression, make_circles, make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "\n",
    "X, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n",
    "\n",
    "plt.scatter(X[np.where((y==0)),0],X[np.where((y==0)),1], label='0')\n",
    "plt.scatter(X[np.where((y==1)),0],X[np.where((y==1)),1], label='1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbL0GDSNWnfb"
   },
   "outputs": [],
   "source": [
    "# Creación del modelo\n",
    "def build_model2(activation = 'sigmoid', loss='binary_crossentropy'):\n",
    "  model = keras.models.Sequential()\n",
    "  model.add(keras.layers.Dense(4, input_dim=2, activation='relu'))\n",
    "  model.add(keras.layers.Dense(8, activation='relu'))\n",
    "  model.add(keras.layers.Dense(1, activation = activation))\n",
    "  opt = keras.optimizers.SGD(learning_rate=0.01)\n",
    "  model.compile(loss=loss, optimizer=opt, metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "#https://developers.google.com/machine-learning/crash-course/classification/accuracy#:~:text=Accuracy%20is%20one%20metric%20for,predictions%20Total%20number%20of%20predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1QKmOqACWyI7",
    "outputId": "87e25f94-d402-4b6f-8b4a-5e361356e21d"
   },
   "outputs": [],
   "source": [
    "# binary cross entropy:\n",
    "keras.backend.clear_session()\n",
    "# Prerprocesado de los datos\n",
    "n_train = 500\n",
    "train_X, test_X = X[:n_train, :], X[n_train:, :]\n",
    "train_y, test_y = y[:n_train], y[n_train:]\n",
    "model = build_model2()\n",
    "history = model.fit(train_X, train_y, validation_data=(test_X, test_y), epochs=1000, verbose=1)\n",
    "# Evaluación del modelo\n",
    "_, train_acc = model.evaluate(train_X, train_y, verbose=0)\n",
    "_, test_acc = model.evaluate(test_X, test_y, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# Gráficas:\n",
    "plt.figure(figsize=(14,7))\n",
    "# Gráfica de pérdidas durante el entrenamiento\n",
    "plt.subplot(121)\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "# Gráfica de las precisión durante el entrenamiento\n",
    "plt.subplot(122)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2g0mp10CVxMc"
   },
   "source": [
    "# Laboratorio\n",
    "1. Para el dataset make a moon de sklearn, construir un modelo de una red neuronal con keras para clasificar los datos.\n",
    "\n",
    "2. Para el dataset load digits, construir un modelo de red neuronal empleando keras para realizar la clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9w2DR2OZV7C3"
   },
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNoVOVgRCUeB"
   },
   "outputs": [],
   "source": [
    "digits = load_digits(n_class=6)\n",
    "X, y = digits.data, digits.target\n",
    "n_samples, n_features = X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "kxSP6-XlD7BU",
    "outputId": "242ac3ac-a2b5-44e4-aca5-757a5c376c48"
   },
   "outputs": [],
   "source": [
    "plt.imshow(X[0].reshape(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "id": "mg8nQgMrDFLY",
    "outputId": "8d734b4b-edfb-4220-86f4-3e5cf1866a6e"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=10, ncols=10, figsize=(6, 6))\n",
    "for idx, ax in enumerate(axs.ravel()):\n",
    "    ax.imshow(X[idx].reshape((8, 8)), cmap=plt.cm.binary)\n",
    "    ax.axis(\"off\")\n",
    "_ = fig.suptitle(\"A selection from the 64-dimensional digits dataset\", fontsize=16)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjZfqUuyEW11"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
